import requests
import time
import random
import re
import pickle
import os
from datetime import datetime

from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException
import undetected_chromedriver as uc
from config import (
    HEADERS, AVITO_SEARCH_URL, TARGET_METRO_STATIONS,
    FILTER_CRITERIA, PROXY_HOST, PROXY_PORT, PROXY_USER, PROXY_PASS
)



class AdvancedAvitoScraper:
    def __init__(self):
        self.headers = HEADERS.copy()
        self.base_url = "https://www.avito.ru"
        self.session = requests.Session()
        self.driver = None
        self.cookies_file = "avito_cookies.pkl"
        self.proxy_index = 0
        self.blocked_proxies = set()
        self.ip_blocked = False
        self.last_block_time = 0
        self.block_count = 0

        # –ü—Ä–æ–∫—Å–∏ —Å–ø–∏—Å–æ–∫ (–¥–æ–±–∞–≤—å—Ç–µ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ–∫—Å–∏)
        self.proxies = [
            # –§–æ—Ä–º–∞—Ç: {'host': 'ip', 'port': 'port', 'username': 'user', 'password': 'pass'}
            {'host': PROXY_HOST, 'port': PROXY_PORT, 'username': PROXY_USER, 'password': PROXY_PASS},
        ]
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        ]

        print(f"[AdvancedScraper] üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å {len(self.proxies)} –ø—Ä–æ–∫—Å–∏")

    def setup_driver(self, use_proxy=True):
        """‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥—Ä–∞–π–≤–µ—Ä–∞ —Å –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–æ–∫—Å–∏"""
        try:
            options = uc.ChromeOptions()

            # –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument('--disable-extensions')

            # User-Agent
            user_agent = random.choice(self.user_agents)
            options.add_argument(f'--user-agent={user_agent}')

            # ‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–∫—Å–∏ –¥–ª—è Selenium
            if use_proxy and self.proxies:
                proxy = self.get_next_proxy()
                if proxy:
                    proxy_url = self.format_proxy_url(proxy)
                    options.add_argument(f'--proxy-server={proxy_url}')
                    print(f"[AdvancedScraper] üåê Selenium –ø—Ä–æ–∫—Å–∏: {proxy['host']}:{proxy['port']}")
                    self.current_proxy = proxy

            options.add_argument('--window-size=1280,720')

            # –°–æ–∑–¥–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä
            self.driver = uc.Chrome(options=options)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º cookies
            self.load_cookies()

            print(f"[AdvancedScraper] ‚úÖ –î—Ä–∞–π–≤–µ—Ä –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–æ–∫—Å–∏")
            return True

        except Exception as e:
            print(f"[AdvancedScraper] ‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥—Ä–∞–π–≤–µ—Ä–∞: {e}")
            return False

    def get_next_proxy(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –ø—Ä–æ–∫—Å–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º"""
        if not self.proxies:
            return None

        available_proxies = [p for p in self.proxies
                             if f"{p['host']}:{p['port']}" not in self.blocked_proxies]

        if not available_proxies:
            print("[AdvancedScraper] ‚ö†Ô∏è –í—Å–µ –ø—Ä–æ–∫—Å–∏ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã, —Å–±—Ä–æ—Å —Å–ø–∏—Å–∫–∞")
            self.blocked_proxies.clear()
            available_proxies = self.proxies

        return random.choice(available_proxies)

    def format_proxy_url(self, proxy):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∫—Å–∏ URL —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π"""
        if 'username' in proxy and 'password' in proxy:
            return f"http://{proxy['username']}:{proxy['password']}@{proxy['host']}:{proxy['port']}"
        else:
            return f"http://{proxy['host']}:{proxy['port']}"

    def load_cookies(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö cookies"""
        if os.path.exists(self.cookies_file):
            try:
                if self.driver.current_url == 'data:,':
                    self.driver.get("https://www.avito.ru")
                    time.sleep(2)

                with open(self.cookies_file, 'rb') as f:
                    cookies = pickle.load(f)
                    for cookie in cookies:
                        try:
                            self.driver.add_cookie(cookie)
                        except:
                            continue
                print(f"[AdvancedScraper] üç™ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(cookies)} cookies")
            except Exception as e:
                print(f"[AdvancedScraper] ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ cookies: {e}")

    def save_cookies(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö cookies"""
        if self.driver:
            try:
                cookies = self.driver.get_cookies()
                with open(self.cookies_file, 'wb') as f:
                    pickle.dump(cookies, f)
                print(f"[AdvancedScraper] üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(cookies)} cookies")
            except Exception as e:
                print(f"[AdvancedScraper] ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è cookies: {e}")

    def check_blocking(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É"""
        if not self.driver:
            return False

        page_source = self.driver.page_source.lower()

        blocking_indicators = [
            '–ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏',
            'captcha',
            '–¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω',
            '–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω',
            'robot',
            'bot',
            '–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å',
            '–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å'
        ]

        for indicator in blocking_indicators:
            if indicator in page_source:
                print(f"[AdvancedScraper] üö´ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞: {indicator}")
                return True

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        try:
            current_url = self.driver.current_url
            if 'blocked' in current_url or 'captcha' in current_url:
                return True
        except:
            pass

        return False

    def handle_blocking(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏"""
        self.ip_blocked = True
        self.last_block_time = time.time()
        self.block_count += 1

        print(f"[AdvancedScraper] üö´ IP –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω (–±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ #{self.block_count})")

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π –¥—Ä–∞–π–≤–µ—Ä
        if self.driver:
            self.driver.quit()
            self.driver = None

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–∫—Å–∏ –≤ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
        if hasattr(self, 'current_proxy'):
            proxy_string = f"{self.current_proxy['host']}:{self.current_proxy['port']}"
            self.blocked_proxies.add(proxy_string)
            print(f"[AdvancedScraper] ‚ùå –ü—Ä–æ–∫—Å–∏ {proxy_string} –¥–æ–±–∞–≤–ª–µ–Ω –≤ —á–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –±–ª–æ–∫–∏—Ä–æ–≤–∫–µ –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
        return {
            'blocked': True,
            'block_count': self.block_count,
            'timestamp': datetime.now(),
            'blocked_proxies': len(self.blocked_proxies)
        }

    def get_apartments(self):
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–≤–∞—Ä—Ç–∏—Ä"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            if self.ip_blocked and (time.time() - self.last_block_time) < 1800:  # 30 –º–∏–Ω—É—Ç
                remaining = 1800 - (time.time() - self.last_block_time)
                print(f"[AdvancedScraper] ‚è∞ –ñ–¥–µ–º —Å–Ω—è—Ç–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏: {remaining / 60:.1f} –º–∏–Ω")
                return []

            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä
            if not self.driver or self.ip_blocked:
                if not self.setup_driver():
                    print("[AdvancedScraper] ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –¥—Ä–∞–π–≤–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                    return self.get_apartments_fallback()
                self.ip_blocked = False

            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
            url = AVITO_SEARCH_URL if AVITO_SEARCH_URL else "https://www.avito.ru/moskva/kvartiry/sdam"
            print(f"[AdvancedScraper] üåê –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞: {url[:80]}...")

            self.driver.get(url)
            time.sleep(random.uniform(3, 5))

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            if self.check_blocking():
                return [self.handle_blocking()]

            # ‚úÖ –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏
            try:
                WebDriverWait(self.driver, 10).until(
                    lambda driver: len(driver.find_elements(By.CSS_SELECTOR, '[data-marker="item"]')) > 0
                )
            except TimeoutException:
                print("[AdvancedScraper] ‚ö†Ô∏è –¢–∞–π–º–∞—É—Ç, –ø—Ä–æ–±—É–µ–º –ø–∞—Ä—Å–∏—Ç—å —á—Ç–æ –µ—Å—Ç—å")

            # –ü–∞—Ä—Å–∏–º
            apartments = self.parse_apartments()
            self.save_cookies()

            print(f"[AdvancedScraper] ‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–≤–∞—Ä—Ç–∏—Ä: {len(apartments)}")
            return apartments

        except Exception as e:
            print(f"[AdvancedScraper] ‚ùå –û—à–∏–±–∫–∞: {e}")
            return self.get_apartments_fallback()

    def parse_apartments(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–≤–∞—Ä—Ç–∏—Ä —Å –ø–æ–º–æ—â—å—é Selenium"""
        apartments = []

        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            apartment_elements = self.driver.find_elements(By.CSS_SELECTOR, '[data-marker="item"]')
            print(f"[AdvancedScraper] üè† –ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(apartment_elements)}")

            for i, element in enumerate(apartment_elements[:15]):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                try:
                    print(f"[AdvancedScraper] üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–∞ {i + 1}")

                    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                    title_elem = element.find_element(By.CSS_SELECTOR, '[data-marker="item-title"]')
                    title = title_elem.text.strip() if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

                    # –°—Å—ã–ª–∫–∞
                    url = title_elem.get_attribute('href') if title_elem else ""

                    # –¶–µ–Ω–∞
                    try:
                        price_elem = element.find_element(By.CSS_SELECTOR, '[data-marker="item-price"]')
                        price = price_elem.text.strip()
                        price_num = self.extract_price_number(price)
                    except:
                        price = "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
                        price_num = 0

                    # –ê–¥—Ä–µ—Å
                    try:
                        address_elem = element.find_element(By.CSS_SELECTOR, '[data-marker="item-address"]')
                        location = address_elem.text.strip()
                    except:
                        location = "–ê–¥—Ä–µ—Å –Ω–µ —É–∫–∞–∑–∞–Ω"

                    # –û–ø–∏—Å–∞–Ω–∏–µ
                    try:
                        desc_elem = element.find_element(By.CSS_SELECTOR, '[data-marker="item-specific-params"]')
                        description = desc_elem.text.strip()
                    except:
                        description = ""

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    rooms, area = self.extract_apartment_params(title, description)
                    metro_info = self.extract_metro_info(element.text)

                    apartment_data = {
                        'title': title,
                        'price': price,
                        'price_num': price_num,
                        'location': location,
                        'metro_info': metro_info,
                        'url': url,
                        'description': description,
                        'rooms': rooms,
                        'area': area,
                        'listing_age': "üìÖ –ù–µ–¥–∞–≤–Ω–æ"
                    }

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏
                    if self.meets_criteria(apartment_data):
                        apartments.append(apartment_data)
                        print(f"[AdvancedScraper] ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ: {title[:50]}...")

                    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏
                    time.sleep(random.uniform(0.3, 0.8))

                except Exception as e:
                    print(f"[AdvancedScraper] ‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ {i + 1}: {e}")
                    continue

        except Exception as e:
            print(f"[AdvancedScraper] ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")

        return apartments

    def extract_price_number(self, price_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è —Ü–µ–Ω—ã"""
        try:
            numbers = re.findall(r'\d+', price_text.replace(' ', ''))
            if numbers:
                return int(''.join(numbers))
        except:
            pass
        return 0

    def extract_apartment_params(self, title, description):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–≤–∞—Ä—Ç–∏—Ä—ã"""
        rooms = None
        area = None

        # –ö–æ–º–Ω–∞—Ç—ã
        if '—Å—Ç—É–¥–∏—è' in title.lower():
            rooms = 0
        else:
            room_match = re.search(r'(\d+)-–∫', title.lower())
            if room_match:
                rooms = int(room_match.group(1))

        # –ü–ª–æ—â–∞–¥—å
        area_match = re.search(r'(\d+(?:[.,]\d+)?)\s*–º¬≤', (title + ' ' + description))
        if area_match:
            area = float(area_match.group(1).replace(',', '.'))

        return rooms, area

    def extract_metro_info(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Ç—Ä–æ"""
        metro_info = {'stations': [], 'time': None}
        text_lower = text.lower()

        # –í—Ä–µ–º—è –¥–æ –º–µ—Ç—Ä–æ
        time_match = re.search(r'(\d+)\s*–º–∏–Ω', text_lower)
        if time_match:
            metro_info['time'] = int(time_match.group(1))

        # –°—Ç–∞–Ω—Ü–∏–∏ –º–µ—Ç—Ä–æ
        for station in TARGET_METRO_STATIONS:
            if station in text_lower:
                metro_info['stations'].append(station)

        return metro_info

    def meets_criteria(self, apartment_data):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤"""
        try:
            # –¶–µ–Ω–∞
            if apartment_data.get('price_num') and apartment_data['price_num'] > FILTER_CRITERIA['max_price']:
                return False

            # –ü–ª–æ—â–∞–¥—å
            if apartment_data.get('area') and apartment_data['area'] < FILTER_CRITERIA['min_area']:
                return False

            # –ö–æ–º–Ω–∞—Ç—ã
            if apartment_data.get('rooms') and apartment_data['rooms'] not in FILTER_CRITERIA['rooms']:
                return False

            # –í—Ä–µ–º—è –¥–æ –º–µ—Ç—Ä–æ
            metro_time = apartment_data['metro_info'].get('time')
            if metro_time and metro_time > FILTER_CRITERIA['max_metro_time']:
                return False

            # –°—Ç–∞–Ω—Ü–∏–∏ –º–µ—Ç—Ä–æ
            metro_stations = apartment_data['metro_info'].get('stations', [])
            if metro_stations:
                return any(station in TARGET_METRO_STATIONS for station in metro_stations)

            # –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ
            full_text = (apartment_data.get('title', '') + ' ' +
                         apartment_data.get('description', '') + ' ' +
                         apartment_data.get('location', '')).lower()

            return any(station in full_text for station in TARGET_METRO_STATIONS)

        except Exception as e:
            print(f"[AdvancedScraper] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {e}")
            return False

    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.driver:
            self.save_cookies()
            self.driver.quit()
            print("[AdvancedScraper] üßπ –†–µ—Å—É—Ä—Å—ã –æ—á–∏—â–µ–Ω—ã")

    def get_apartments_fallback(self):
        """Fallback –Ω–∞ requests —Å –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º –ø—Ä–æ–∫—Å–∏"""
        print("[AdvancedScraper] üîÑ Fallback –Ω–∞ requests —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π...")

        try:
            url = AVITO_SEARCH_URL if AVITO_SEARCH_URL else "https://www.avito.ru/moskva/kvartiry/sdam"

            # ‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–∫—Å–∏ –¥–ª—è requests
            proxies_dict = None
            if self.proxies:
                proxy = self.get_next_proxy()
                if proxy:
                    proxy_url = self.format_proxy_url(proxy)
                    proxies_dict = {
                        'http': proxy_url,
                        'https': proxy_url
                    }
                    print(
                        f"[AdvancedScraper] üåê Requests –ø—Ä–æ–∫—Å–∏: {proxy['username']}:***@{proxy['host']}:{proxy['port']}")

            # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å
            response = self.session.get(
                url,
                headers=self.headers,
                proxies=proxies_dict,
                timeout=15
            )

            if response.status_code == 429:
                return [self.handle_blocking()]

            if response.status_code != 200:
                print(f"[AdvancedScraper] ‚ùå HTTP {response.status_code}")
                return []

            # –ü–∞—Ä—Å–∏–º —á–µ—Ä–µ–∑ BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            cards = soup.find_all('div', {'data-marker': 'item'})

            apartments = []
            for card in cards[:5]:
                try:
                    apartment_data = self.parse_card_with_bs4(card)
                    if apartment_data and self.meets_criteria(apartment_data):
                        apartments.append(apartment_data)
                except:
                    continue

            print(f"[AdvancedScraper] üìä Fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {len(apartments)} –∫–≤–∞—Ä—Ç–∏—Ä")
            return apartments

        except Exception as e:
            print(f"[AdvancedScraper] ‚ùå Fallback –æ—à–∏–±–∫–∞: {e}")
            return []

    def parse_card_with_bs4(self, card):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ä—Ç–æ—á–∫–∏ —á–µ—Ä–µ–∑ BeautifulSoup"""
        try:
            title_elem = card.find('a', {'data-marker': 'item-title'})
            title = title_elem.text.strip() if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

            price_elem = card.find('span', {'data-marker': 'item-price'})
            price = price_elem.text.strip() if price_elem else "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
            price_num = self.extract_price_number(price)

            address_elem = card.find('div', {'data-marker': 'item-address'})
            location = address_elem.text.strip() if address_elem else "–ê–¥—Ä–µ—Å –Ω–µ —É–∫–∞–∑–∞–Ω"

            url = title_elem.get('href', '') if title_elem else ''
            if url and not url.startswith('http'):
                url = self.base_url + url

            description = card.get_text()
            rooms, area = self.extract_apartment_params(title, description)
            metro_info = self.extract_metro_info(description)

            return {
                'title': title,
                'price': price,
                'price_num': price_num,
                'location': location,
                'metro_info': metro_info,
                'url': url,
                'description': description,
                'rooms': rooms,
                'area': area,
                'listing_age': "üìÖ –ù–µ–¥–∞–≤–Ω–æ"
            }
        except Exception as e:
            print(f"[AdvancedScraper] ‚ùå BS4 –ø–∞—Ä—Å–∏–Ω–≥: {e}")
            return None
