import requests
from bs4 import BeautifulSoup
import time
import random
import re
import json
from datetime import datetime, timedelta
from config import HEADERS, AVITO_SEARCH_URL, TARGET_METRO_STATIONS, FILTER_CRITERIA


class AvitoScraper:
    def __init__(self):
        self.headers = HEADERS
        self.base_url = "https://www.avito.ru"
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0',
        ]

        self.request_count = 0
        self.last_request_time = 0
        self.session = requests.Session()

    print(f"[Scraper] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏: {FILTER_CRITERIA}")

    def get_apartments(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–≤–∞—Ä—Ç–∏—Ä —Å Avito —á–µ—Ä–µ–∑ API"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π API Avito –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
            api_url = self.convert_search_url_to_api()
            print(f"[Scraper] API –∑–∞–ø—Ä–æ—Å: {api_url}")

            self.smart_delay()
            self.rotate_user_agent()

            response = requests.get(api_url, headers=self.headers)
            print(f"[Scraper] –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {response.status_code}")
            response.raise_for_status()

            data = response.json()
            print(f"[Scraper] –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç, –∫–ª—é—á–∏: {list(data.keys())}")

            apartments = []

            if 'items' in data:
                items = data['items']
                print(f"[Scraper] –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –≤ API: {len(items)}")

                for i, item in enumerate(items):
                    print(f"\n[Scraper] === –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è {i + 1}/{len(items)} ===")
                    apartment_data = self.parse_apartment_from_api(item)

                    if apartment_data:
                        print(f"[Scraper] ‚úÖ –û–±—ä—è–≤–ª–µ–Ω–∏–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: '{apartment_data['title'][:50]}...'")
                        print(
                            f"[Scraper] –¶–µ–Ω–∞: {apartment_data['price']}, –ü–ª–æ—â–∞–¥—å: {apartment_data.get('area', '–Ω–µ —É–∫–∞–∑–∞–Ω–∞')} –º¬≤")
                        print(
                            f"[Scraper] –ú–µ—Ç—Ä–æ: {apartment_data['metro_info']['stations'][:2] if apartment_data['metro_info']['stations'] else '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}")

                        if self.meets_criteria(apartment_data):
                            apartments.append(apartment_data)
                            print(f"[Scraper] üéØ –ü–û–î–•–û–î–ò–¢! –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ —Å–ø–∏—Å–æ–∫")
                        else:
                            print(f"[Scraper] ‚ùå –ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º")
                    else:
                        print(f"[Scraper] ‚ö†Ô∏è –û–±—ä—è–≤–ª–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å")

            else:
                print(f"[Scraper] ‚ùå –ö–ª—é—á 'items' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ API")
                print(f"[Scraper] –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {list(data.keys())}")
                if 'error' in data:
                    print(f"[Scraper] –û—à–∏–±–∫–∞ API: {data['error']}")

            print(f"\n[Scraper] üìä –ò—Ç–æ–≥–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∫–≤–∞—Ä—Ç–∏—Ä: {len(apartments)} –∏–∑ {len(data.get('items', []))}")
            return apartments

        except requests.exceptions.RequestException as e:
            print(f"[Scraper] ‚ùå –û—à–∏–±–∫–∞ HTTP –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return self.get_apartments_html_fallback()
        except json.JSONDecodeError as e:
            print(f"[Scraper] ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return self.get_apartments_html_fallback()
        except Exception as e:
            print(f"[Scraper] ‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å API: {e}")
            return self.get_apartments_html_fallback()

    def convert_search_url_to_api(self):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è URL –ø–æ–∏—Å–∫–∞ –≤ API –∑–∞–ø—Ä–æ—Å"""
        # –ï—Å–ª–∏ –∑–∞–¥–∞–Ω AVITO_SEARCH_URL, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        if AVITO_SEARCH_URL and AVITO_SEARCH_URL.strip():
            print(f"[Scraper] –ò—Å–ø–æ–ª—å–∑—É–µ–º URL –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è: {AVITO_SEARCH_URL}")
            return AVITO_SEARCH_URL

        # –ò–Ω–∞—á–µ —Ñ–æ—Ä–º–∏—Ä—É–µ–º API URL
        api_url = "https://www.avito.ru/web/1/main/items"

        params = {
            'locationId': '637640',  # –ú–æ—Å–∫–≤–∞
            'categoryId': '24',  # –ö–≤–∞—Ä—Ç–∏—Ä—ã
            'params[549]': '1059',  # –ê—Ä–µ–Ω–¥–∞
            'params': '1,2',
            'priceMax': str(FILTER_CRITERIA['max_price']),
            'areaMin': str(FILTER_CRITERIA['min_area']),
            'sort': 'date',
            'limit': '50',
            'page': '1'
        }

        param_string = '&'.join([f"{k}={v}" for k, v in params.items()])
        full_url = f"{api_url}?{param_string}"

        print(f"[Scraper] –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π API URL: {full_url}")
        return full_url

    def parse_apartment_from_api(self, item):
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–≤–∞—Ä—Ç–∏—Ä—ã –∏–∑ API –æ—Ç–≤–µ—Ç–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç—ã - –°–ï–ì–û–î–ù–Ø –ò –í–ß–ï–†–ê
            if not self.is_recent_listing(item):
                print(f"[Scraper] ‚è∞ –û–±—ä—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ –ø–æ –¥–∞—Ç–µ: {item.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")
                return None

            apartment_id = item.get('id', '')
            title = item.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã
            price_info = item.get('priceDetailing', {})
            price_value = price_info.get('value', 0)
            price_text = f"{price_value:,} ‚ÇΩ" if price_value else "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è –∏ –º–µ—Ç—Ä–æ
            location_info = item.get('location', {})
            location = location_info.get('name', '–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ')

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Ç—Ä–æ
            metro_info = self.extract_metro_from_api_item(item)

            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ URL
            url = f"https://www.avito.ru{item.get('urlPath', '')}"

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            images = item.get('images', [])
            image_url = images[0].get('636x476') if images else ""

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            description = item.get('description', '')
            rooms, area = self.extract_apartment_params_from_api(item)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
            listing_age = self.get_listing_age(item)

            return {
                'id': apartment_id,
                'title': title,
                'price': price_text,
                'price_num': price_value,
                'location': location,
                'metro_info': metro_info,
                'url': url,
                'image_url': image_url,
                'description': description,
                'rooms': rooms,
                'area': area,
                'date_published': item.get('sortTimeStamp', 0),
                'listing_age': listing_age
            }

        except Exception as e:
            print(f"[Scraper] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ API: {e}")
            return None

    def is_recent_listing(self, item):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ —Å–µ–≥–æ–¥–Ω—è –∏–ª–∏ –≤—á–µ—Ä–∞"""
        try:
            timestamp = item.get('sortTimeStamp', 0)
            if timestamp:
                listing_date = datetime.fromtimestamp(timestamp)
                today = datetime.now().date()
                yesterday = today - timedelta(days=1)

                listing_day = listing_date.date()
                is_recent = listing_day == today or listing_day == yesterday
                print(f"[Scraper] –î–∞—Ç–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {listing_day}, —Å–µ–≥–æ–¥–Ω—è: {today}, –ø–æ–¥—Ö–æ–¥–∏—Ç: {is_recent}")
                return is_recent
            print(f"[Scraper] ‚ö†Ô∏è –í—Ä–µ–º–µ–Ω–Ω–∞—è –º–µ—Ç–∫–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            return False
        except Exception as e:
            print(f"[Scraper] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞—Ç—ã: {e}")
            return False

    def get_listing_age(self, item):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            timestamp = item.get('sortTimeStamp', 0)
            if timestamp:
                listing_date = datetime.fromtimestamp(timestamp)
                today = datetime.now().date()

                if listing_date.date() == today:
                    hours_ago = (datetime.now() - listing_date).total_seconds() / 3600
                    if hours_ago < 1:
                        return "üìÖ –¢–æ–ª—å–∫–æ —á—Ç–æ"
                    elif hours_ago < 24:
                        return f"üìÖ {int(hours_ago)} —á –Ω–∞–∑–∞–¥"
                    else:
                        return "üìÖ –°–µ–≥–æ–¥–Ω—è"
                else:
                    return "üìÖ –í—á–µ—Ä–∞"
            return "üìÖ –ù–µ–¥–∞–≤–Ω–æ"
        except:
            return "üìÖ –ù–µ–¥–∞–≤–Ω–æ"

    def extract_metro_from_api_item(self, item):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Ç—Ä–æ –∏–∑ API"""
        metro_info = {'stations': [], 'time': None}

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö API –æ—Ç–≤–µ—Ç–∞
            geo = item.get('geo', {})
            references = geo.get('references', [])

            for ref in references:
                ref_text = str(ref).lower()

                # –ü–æ–∏—Å–∫ –≤—Ä–µ–º–µ–Ω–∏ –¥–æ –º–µ—Ç—Ä–æ
                time_match = re.search(r'(\d+)\s*–º–∏–Ω', ref_text)
                if time_match:
                    metro_info['time'] = int(time_match.group(1))

                # –ü–æ–∏—Å–∫ —Å—Ç–∞–Ω—Ü–∏–π –º–µ—Ç—Ä–æ
                for station in TARGET_METRO_STATIONS:
                    if station in ref_text:
                        metro_info['stations'].append(station)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–µ
            full_text = (item.get('title', '') + ' ' + item.get('description', '')).lower()
            for station in TARGET_METRO_STATIONS:
                if station in full_text and station not in metro_info['stations']:
                    metro_info['stations'].append(station)

        except Exception as e:
            print(f"[Scraper] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Ç—Ä–æ: {e}")

        return metro_info

    def extract_apartment_params_from_api(self, item):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–≤–∞—Ä—Ç–∏—Ä—ã –∏–∑ API"""
        rooms = None
        area = None

        try:
            params = item.get('params', {})

            # –ü–æ–∏—Å–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–Ω–∞—Ç
            if 'rooms' in params:
                rooms = int(params['rooms'])
            else:
                title = item.get('title', '').lower()
                room_match = re.search(r'(\d+)-–∫', title)
                if room_match:
                    rooms = int(room_match.group(1))

            # –ü–æ–∏—Å–∫ –ø–ª–æ—â–∞–¥–∏
            if 'area' in params:
                area = float(params['area'])
            else:
                full_text = item.get('title', '') + ' ' + item.get('description', '')
                area_match = re.search(r'(\d+(?:[.,]\d+)?)\s*–º¬≤', full_text)
                if area_match:
                    area = float(area_match.group(1).replace(',', '.'))

            print(f"[Scraper] –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: –∫–æ–º–Ω–∞—Ç={rooms}, –ø–ª–æ—â–∞–¥—å={area}")

        except Exception as e:
            print(f"[Scraper] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")

        return rooms, area

    def get_apartments_html_fallback(self):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π HTML –ø–∞—Ä—Å–∏–Ω–≥ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏"""
        print(f"[Scraper] üåê –ó–∞–ø—É—Å–∫ HTML –ø–∞—Ä—Å–∏–Ω–≥–∞...")

        try:
            url = AVITO_SEARCH_URL if AVITO_SEARCH_URL else "https://www.avito.ru/moskva/kvartiry/sdam"
            print(f"[Scraper] üì° –ó–∞–ø—Ä–æ—Å –∫: {url[:100]}...")

            # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–µ—Å—Å–∏—é —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
            response = self.session.get(url, headers=self.headers, timeout=30)

            print(f"[Scraper] üìä –°—Ç–∞—Ç—É—Å: {response.status_code}, –†–∞–∑–º–µ—Ä: {len(response.content)} –±–∞–π—Ç")

            # ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            if response.status_code == 429:
                print(f"[Scraper] üö´ –ü–æ–ª—É—á–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ 429, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫–∏...")
                self.request_count += 5  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–ª—è –±–æ–ª—å—à–∏—Ö –∑–∞–¥–µ—Ä–∂–µ–∫
                time.sleep(60)  # –ñ–¥–µ–º –º–∏–Ω—É—Ç—É
                return []

            if response.status_code == 403:
                print(f"[Scraper] üö´ –î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω (403), –≤–æ–∑–º–æ–∂–Ω–æ IP –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω")
                return []

            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            if 'captcha' in response.text.lower() or '–ø—Ä–æ–≤–µ—Ä–∫–∞' in response.text.lower():
                print(f"[Scraper] üîí –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥")
                return []

            apartments = []
            apartment_cards = soup.find_all('div', {'data-marker': 'item'})
            print(f"[Scraper] üè† –ù–∞–π–¥–µ–Ω–æ HTML –∫–∞—Ä—Ç–æ—á–µ–∫: {len(apartment_cards)}")

            if len(apartment_cards) == 0:
                print(f"[Scraper] ‚ö†Ô∏è –ö–∞—Ä—Ç–æ—á–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –≤–æ–∑–º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–ª–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
                # –°–æ—Ö—Ä–∞–Ω–∏–º HTML –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                with open('debug_response.html', 'w', encoding='utf-8') as f:
                    f.write(response.text[:5000])
                print(f"[Scraper] üíæ –ü–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ debug_response.html")

            for i, card in enumerate(apartment_cards[:20], 1):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 20 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏
                print(f"[Scraper] üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ä—Ç–æ—á–∫–∏ {i}/{min(len(apartment_cards), 20)}")
                apartment_data = self.parse_apartment_card_updated(card)

                if apartment_data:
                    if self.meets_criteria(apartment_data):
                        apartments.append(apartment_data)
                        print(f"[Scraper] ‚úÖ –ö–≤–∞—Ä—Ç–∏—Ä–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∞: {apartment_data['title'][:50]}...")
                    else:
                        print(f"[Scraper] ‚ùå –ù–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º")

                time.sleep(random.uniform(0.5, 1.5))

            print(f"[Scraper] üìà –ò—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∫–≤–∞—Ä—Ç–∏—Ä: {len(apartments)}")
            return apartments

        except requests.exceptions.RequestException as e:
            print(f"[Scraper] ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            if '429' in str(e):
                print(f"[Scraper] üö´ –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫–∏...")
                self.request_count += 10
                time.sleep(120)  # –ñ–¥–µ–º 2 –º–∏–Ω—É—Ç—ã
            return []
        except Exception as e:
            print(f"[Scraper] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ HTML –ø–∞—Ä—Å–∏–Ω–≥–µ: {e}")
            return []

    def parse_apartment_card_updated(self, card):
        """–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ HTML –∫–∞—Ä—Ç–æ—á–∫–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏"""
        try:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            title_elem = (card.find('a', {'data-marker': 'item-title'}) or
                          card.find('h3') or
                          card.find('a', class_=re.compile('title')))
            title = title_elem.text.strip() if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

            # –¶–µ–Ω–∞ - —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            price_elem = (card.find('span', {'data-marker': 'item-price'}) or
                          card.find('meta', {'itemprop': 'price'}) or
                          card.find('span', class_=re.compile('price')))

            if price_elem:
                if price_elem.name == 'meta':
                    price_value = int(price_elem.get('content', 0))
                    price = f"{price_value:,} ‚ÇΩ"
                else:
                    price = price_elem.text.strip()
                    price_value = self.extract_price_number(price)
            else:
                price = "–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"
                price_value = 0

            # –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ
            address_elem = (card.find('div', {'data-marker': 'item-address'}) or
                            card.find('span', class_=re.compile('address')))
            location = address_elem.text.strip() if address_elem else "–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ"

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–µ—Ç—Ä–æ - —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
            metro_info = self.extract_metro_from_html(card)

            # –°—Å—ã–ª–∫–∞
            url = ""
            if title_elem and title_elem.get('href'):
                href = title_elem['href']
                url = href if href.startswith('http') else self.base_url + href

            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img_elem = (card.find('img', {'data-marker': 'item-photo'}) or
                        card.find('img'))
            image_url = img_elem.get('src', '') if img_elem else ""

            # –û–ø–∏—Å–∞–Ω–∏–µ - –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
            desc_elem = (card.find('div', {'data-marker': 'item-specific-params'}) or
                         card.find('p', class_=re.compile('description')) or
                         card.find('div', class_=re.compile('description')))
            description = desc_elem.text.strip() if desc_elem else ""

            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã
            rooms, area = self.extract_apartment_params(title, description)

            return {
                'title': title,
                'price': price,
                'price_num': price_value,
                'location': location,
                'metro_info': metro_info,
                'url': url,
                'image_url': image_url,
                'description': description,
                'rooms': rooms,
                'area': area,
                'listing_age': "üìÖ –ù–µ–¥–∞–≤–Ω–æ"
            }

        except Exception as e:
            print(f"[Scraper] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–∞—Ä—Ç–æ—á–∫–∏: {e}")
            return None

    def extract_metro_from_html(self, card):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–µ—Ç—Ä–æ –∏–∑ HTML"""
        metro_info = {'stations': [], 'time': None}

        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞—Ä—Ç–æ—á–∫–∏
            full_text = card.get_text().lower()

            # –ü–æ–∏—Å–∫ –≤—Ä–µ–º–µ–Ω–∏ –¥–æ –º–µ—Ç—Ä–æ —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏
            time_patterns = [
                r'(\d+)\s*–º–∏–Ω(?:—É—Ç)?(?:\s*–¥–æ\s*–º(?:–µ—Ç—Ä–æ|\.)?)',
                r'(\d+)\s*–º–∏–Ω(?:—É—Ç)?(?:\s*–ø–µ—à–∫–æ–º)',
                r'(\d+)\s*–º–∏–Ω(?:—É—Ç)?(?:\s*–º\.)',
                r'(\d+)\s*–º\.',
            ]

            for pattern in time_patterns:
                matches = re.findall(pattern, full_text)
                if matches:
                    metro_info['time'] = min(int(t) for t in matches)
                    print(f"[Scraper] ‚è∞ –ù–∞–π–¥–µ–Ω–æ –≤—Ä–µ–º—è –¥–æ –º–µ—Ç—Ä–æ: {metro_info['time']} –º–∏–Ω")
                    break

            # –ü–æ–∏—Å–∫ —Å—Ç–∞–Ω—Ü–∏–π –º–µ—Ç—Ä–æ —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏
            found_stations = []
            for station in TARGET_METRO_STATIONS:
                patterns = [
                    rf'\b{re.escape(station)}\b',
                    rf'\b–º\.\s*{re.escape(station)}\b',
                    rf'\b–º–µ—Ç—Ä–æ\s+{re.escape(station)}\b',
                    rf'\b{re.escape(station)}\s+–º–µ—Ç—Ä–æ\b',
                ]

                for pattern in patterns:
                    if re.search(pattern, full_text, re.IGNORECASE):
                        if station not in found_stations:
                            found_stations.append(station)
                            print(f"[Scraper] üéØ –ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞–Ω—Ü–∏—è: {station}")
                        break

            metro_info['stations'] = found_stations

        except Exception as e:
            print(f"[Scraper] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ—Ç—Ä–æ: {e}")

        return metro_info

    def extract_apartment_params(self, title, description):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–Ω–∞—Ç –∏ –ø–ª–æ—â–∞–¥–∏"""
        rooms = None
        area = None

        room_patterns = [r'(\d+)-–∫', r'(\d+)\s*–∫–æ–º–Ω', r'(\d+)-–∫–æ–º–Ω']

        for pattern in room_patterns:
            match = re.search(pattern, title.lower())
            if match:
                rooms = int(match.group(1))
                break

        area_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*–º¬≤',
            r'(\d+(?:[.,]\d+)?)\s*–∫–≤\.?–º',
            r'–ø–ª–æ—â–∞–¥—å[:\s]+(\d+(?:[.,]\d+)?)'
        ]

        full_text = title + ' ' + description
        for pattern in area_patterns:
            match = re.search(pattern, full_text.lower())
            if match:
                area = float(match.group(1).replace(',', '.'))
                break

        return rooms, area

    def extract_price_number(self, price_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è —Ü–µ–Ω—ã"""
        try:
            numbers = re.findall(r'\d+', price_text.replace(' ', ''))
            if numbers:
                return int(''.join(numbers))
        except:
            pass
        return None

    def meets_criteria(self, apartment_data):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫—Ä–∏—Ç–µ—Ä–∏—è–º —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        try:
            print(f"[Scraper] üîç –ü—Ä–æ–≤–µ—Ä–∫–∞: {apartment_data['title'][:50]}...")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–Ω—ã
            if apartment_data.get('price_num') and apartment_data['price_num'] > FILTER_CRITERIA['max_price']:
                print(f"[Scraper] ‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–∞ —Ü–µ–Ω–∞: {apartment_data['price']} > {FILTER_CRITERIA['max_price']}")
                return False

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–ª–æ—â–∞–¥–∏
            if apartment_data.get('area') and apartment_data['area'] < FILTER_CRITERIA['min_area']:
                print(f"[Scraper] ‚ùå –ú–∞–ª–æ –ø–ª–æ—â–∞–¥–∏: {apartment_data['area']} < {FILTER_CRITERIA['min_area']}")
                return False

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–º–Ω–∞—Ç
            if apartment_data.get('rooms') and apartment_data['rooms'] not in FILTER_CRITERIA['rooms']:
                print(
                    f"[Scraper] ‚ùå –ù–µ–ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç: {apartment_data['rooms']} –Ω–µ –≤ {FILTER_CRITERIA['rooms']}")
                return False

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –¥–æ –º–µ—Ç—Ä–æ
            metro_time = apartment_data['metro_info'].get('time')
            if metro_time and metro_time > FILTER_CRITERIA['max_metro_time']:
                print(f"[Scraper] ‚ùå –î–∞–ª–µ–∫–æ –¥–æ –º–µ—Ç—Ä–æ: {metro_time} –º–∏–Ω > {FILTER_CRITERIA['max_metro_time']}")
                return False

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–Ω—Ü–∏–π –º–µ—Ç—Ä–æ
            metro_stations = apartment_data['metro_info'].get('stations', [])

            if metro_stations:
                matches = [s for s in metro_stations if s in TARGET_METRO_STATIONS]
                if matches:
                    print(f"[Scraper] ‚úÖ –ü–æ–¥—Ö–æ–¥—è—â–∏–µ —Å—Ç–∞–Ω—Ü–∏–∏: {matches}")
                    print(f"[Scraper] ‚úÖ –í–°–ï –ö–†–ò–¢–ï–†–ò–ò –ü–†–û–ô–î–ï–ù–´!")
                    return True
                else:
                    print(f"[Scraper] ‚ùå –ù–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å—Ç–∞–Ω—Ü–∏–∏: {metro_stations}")
                    return False
            else:
                # –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ
                full_text = (apartment_data.get('title', '') + ' ' +
                             apartment_data.get('description', '') + ' ' +
                             apartment_data.get('location', '')).lower()

                found_stations = []
                for station in TARGET_METRO_STATIONS:
                    if station in full_text:
                        found_stations.append(station)

                if found_stations:
                    print(f"[Scraper] ‚úÖ –°—Ç–∞–Ω—Ü–∏–∏ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ: {found_stations}")
                    print(f"[Scraper] ‚úÖ –í–°–ï –ö–†–ò–¢–ï–†–ò–ò –ü–†–û–ô–î–ï–ù–´!")
                    return True
                else:
                    print(f"[Scraper] ‚ùå –°—Ç–∞–Ω—Ü–∏–∏ –º–µ—Ç—Ä–æ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                    return False

        except Exception as e:
            print(f"[Scraper] ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤: {e}")
            return False

    def add_random_delay(self):
        """–°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏"""
        delay = random.uniform(5, 12)
        print(f"[Scraper] ‚è≥ –ó–∞–¥–µ—Ä–∂–∫–∞ {delay:.1f} —Å–µ–∫—É–Ω–¥...")
        time.sleep(delay)

    def smart_delay(self):
        """–£–º–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        current_time = time.time()

        # –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
        base_delay = 15 + (self.request_count * 2)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É —Å –∫–∞–∂–¥—ã–º –∑–∞–ø—Ä–æ—Å–æ–º

        # –ï—Å–ª–∏ –ø—Ä–æ—à–ª—ã–π –∑–∞–ø—Ä–æ—Å –±—ã–ª –Ω–µ–¥–∞–≤–Ω–æ, –∂–¥–µ–º –¥–æ–ª—å—à–µ
        time_since_last = current_time - self.last_request_time
        if time_since_last < base_delay:
            additional_delay = base_delay - time_since_last
            print(f"[Scraper] ‚è≥ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: {additional_delay:.1f} —Å–µ–∫")
            time.sleep(additional_delay)

        # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞
        random_delay = random.uniform(5, 15)
        print(f"[Scraper] ‚è∞ –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞: {random_delay:.1f} —Å–µ–∫")
        time.sleep(random_delay)

        self.last_request_time = time.time()
        self.request_count += 1

    def rotate_user_agent(self):
        """–†–æ—Ç–∞—Ü–∏—è User-Agent –∏ –¥—Ä—É–≥–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
        self.headers['User-Agent'] = random.choice(self.user_agents)

        # ‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∫–∏
        self.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': random.choice([
                'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
                'ru-RU,ru;q=0.9,en;q=0.8',
                'ru,en-US;q=0.7,en;q=0.3'
            ]),
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        })

        print(f"[Scraper] üîÑ User-Agent: {self.headers['User-Agent'][:50]}...")
